# 中国多模态大模型研究团队整理报告

## 研究背景与目标

多模态大语言模型（Multimodal Large Language Models，简称MLLMs）作为人工智能领域的前沿研究方向，近年来在中国各大高校和研究机构得到了广泛关注与发展。

## 清华大学研究团队

### 知识工程实验室（KE Lab）
- **负责人**：唐杰教授
- **实验室**：清华大学计算机系知识工程研究室
- **研究方向**：大规模预训练模型、知识图谱构建与推理
- **代表性工作**：
  - **GLM系列模型**：ChatGLM、ChatGLM2、ChatGLM3
  - **VisualGLM**：视觉语言模型
  - **CodeGeeX**：代码生成模型
- **合作机构**：智谱AI
- **招生方向**：大模型预训练技术、多模态表示学习、知识推理与增强

### 孙茂松教授团队
- **研究方向**：自然语言处理、多模态内容生成
- **代表性工作**：九歌诗歌生成系统

### 朱军教授团队
- **研究方向**：机器学习基础理论、生成模型
- **研究特色**：为多模态AI发展提供坚实的理论支撑

## 北京大学研究力量

### 智能科学实验室
- **负责人**：封举富教授
- **研究方向**：计算机视觉、模式识别、多模态学习
- **实验室**：北京大学信息科学技术学院

### 王选计算机研究所
- **研究方向**：多模态AI研究、跨学科交叉

### 人工智能研究院
- **研究方向**：
  - 多模态表示学习
  - 机器学习理论
  - 跨模态理解
- **研究特色**：跨学科合作与创新

## 中国科学院体系

### 自动化研究所
- **研究方向**：计算机视觉、自然语言处理、机器学习
- **代表性工作**：
  - **MiniGPT-4**：参与国际知名开源多模态模型开发
  - 视觉-语言预训练模型研究
- **研究重点**：
  - 多模态表征学习
  - 跨模态注意力机制
  - 视觉编码器与大语言模型连接

### 计算技术研究所
- **研究方向**：大规模AI模型训练、计算架构优化
- **研究目标**：解决多模态模型在实际部署中的工程问题

## 浙江大学研究团队

### 赵思嘉教授团队
- **研究方向**：
  - 视觉Transformer架构设计
  - 多模态表征学习
  - 零样本和少样本学习
- **学术成果**：在顶级会议和期刊发表大量高质量论文

### 人工智能研究所
- **研究方向**：多模态AI与机器人技术交叉研究
- **重点方向**：具身智能、多模态感知、机器人视觉

## 上海人工智能实验室

### InternVL系列
- **代表性模型**：InternVL-Chat、InternVL
- **应用方向**：图像理解、视觉问答、多模态对话
- **研究意义**：标志着中国在多模态大模型领域达到国际先进水平

### InternLM系列
- **代表性模型**：InternLM语言大模型
- **研究基础**：为多模态模型开发提供强大的语言理解基础

### ImageBind
- **研究方向**：多模态嵌入学习
- **技术目标**：将不同模态信息映射到统一的语义空间

## 企业研究力量

### 智谱AI
- **合作机构**：清华大学知识工程实验室
- **代表性模型**：
  - ChatGLM系列
  - VisualGLM
  - CogVLM
- **研究特色**：基础研究和产品化应用并重

### 阿里巴巴达摩院
- **代表性模型**：Qwen-VL、Qwen系列
- **研究方向**：多模态大模型研发、模型实际部署
- **技术特色**：关注模型的实际应用场景

## 研究方向与招生建议

### 核心研究方向
1. 大规模视觉语言预训练模型构建
2. 多模态表示学习与对齐
3. 跨模态推理与生成
4. 具身智能与多模态感知
5. 视觉Transformer架构创新

### 申请建议
1. **基础知识**：系统学习深度学习、机器学习、NLP、计算机视觉
2. **文献阅读**：了解当前多模态大模型发展现状和技术路线
3. **技能要求**：良好的编程能力和数学基础
4. **联系时间**：提前3-6个月联系目标导师

## 重要提醒

- 本报告信息基于公开资料整理
- 建议以各高校和实验室官方网站信息为准
- 多模态大模型领域发展迅速，招生政策可能随时调整
- 请保持关注最新动态

---
*文档生成时间：2026-02-12*
*信息来源：公开网络资料整理*
